{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need for this lab\n",
    "\n",
    "# Using the following line code to install the torchvision library\n",
    "# !conda install -y torchvision\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data\n",
    "\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(28, 28), cmap='gray')\n",
    "    plt.title('y = ' + str(data_sample[1].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Makeup_Data\">Make Some Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the training dataset:\n",
      "  Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "# Create and print the training dataset\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "print(\"Print the training dataset:\\n \", train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the validating dataset:\n",
      "  Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "# Create and print the validating dataset\n",
    "\n",
    "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "print(\"Print the validating dataset:\\n \", validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data element:  torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# Print the type of the element\n",
    "\n",
    "print(\"Type of data element: \", train_dataset[0][1].type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular tensor corresponds to a number that represents a pixel intensity as demonstrated by the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter3/3.32_image_values.png\" width=\"550\" alt=\"MNIST elements\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label:  tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# Print the label\n",
    "\n",
    "print(\"The label: \", train_dataset[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image:  None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " #Plot the image\n",
    "\n",
    "print(\"The image: \", show_data(train_dataset[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Softmax function requires vector inputs. If you see the vector shape, you'll note it's 28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of the first element in train_dataset\n",
    "\n",
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the tensor as shown in this image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter3/3.3.2Imagetovector2.png\" width=\"550\" alt=\"Flattern Image\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Practice</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input size and output size\n",
    "\n",
    "input_dim = 28 * 28\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Create a softmax classifier by using sequenital\n",
    "model = nn.Sequential(nn.Linear(input_dim, output_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Model\">Define the Softmax Classifier, Criterion function, Optimizer, and Train the Model</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the size of the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:  torch.Size([10, 784])\n",
      "b:  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Print the parameters\n",
    "\n",
    "print('W: ', list(model.parameters())[0].size())\n",
    "print('b: ', list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cover the model parameters for each class to a rectangular grid: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>     <img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter3/3.3.2paramaters_to_image.gif\" width = 550, align = \"center\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate, optimizer, criterion and data loader\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "n_epochs = 10\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "N_test = len(validation_dataset)\n",
    "\n",
    "def train_model(n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x.view(-1, 28 * 28))\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        correct = 0\n",
    "        \n",
    "        #perform a prediction on the validation  data  \n",
    "        for x_test, y_test in validation_loader:\n",
    "            z = model(x_test.view(-1, 28 * 28))\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        accuracy = correct / N_test\n",
    "        loss_list.append(loss.data)\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "train_model(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Result\">Analyze Results</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(loss_list, color = color)\n",
    "ax1.set_xlabel('epoch', color = color)\n",
    "ax1.set_ylabel('total loss', color = color)\n",
    "ax1.tick_params(axis = 'y', color = color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color = color)  \n",
    "ax2.plot( accuracy_list, color = color)\n",
    "ax2.tick_params(axis = 'y', color = color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the misclassified samples\n",
    "\n",
    "count = 0\n",
    "for x, y in validation_dataset:\n",
    "    z = model(x.reshape(-1, 28 * 28))\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    if yhat != y:\n",
    "        show_data((x, y))\n",
    "        plt.show()\n",
    "        print(\"yhat: \",yhat)\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai-cpu]",
   "language": "python",
   "name": "conda-env-fastai-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
