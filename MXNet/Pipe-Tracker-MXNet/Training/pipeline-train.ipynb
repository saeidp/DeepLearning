{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'pipe-line-test-2'\n",
    "prefix = 'ss-demo'\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of jpeg images in train and png images in train_annotation must be the same, and so in validation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import glob\n",
    "#num_training_samples=len(glob.glob1('train',\"*.jpg\"))\n",
    "#print ( ' Num Train Images = ' + str(num_training_samples))\n",
    "#assert num_training_samples == len(glob.glob1('train_annotation',\"*.png\"))\n",
    "#print ( ' Num Validation Images = ' + str(len(glob.glob1('validation',\"*.jpg\"))))\n",
    "#assert len(glob.glob1('validation',\"*.jpg\")) == len(glob.glob1('validation_annotation',\"*.png\"))\n",
    "\n",
    "num_training_samples = len(sess.list_s3_files(bucket, prefix + '/train')) // 2\n",
    "print ( ' Num Train Images = ' + str(num_training_samples))\n",
    "\n",
    "num_training_annotation_samples = len(sess.list_s3_files(bucket, prefix + '/train_annotation'))\n",
    "print ( ' Num Train annotation Images = ' + str(num_training_annotation_samples))\n",
    "\n",
    "assert num_training_samples == num_training_annotation_samples\n",
    "\n",
    "num_validation_samples = len(sess.list_s3_files(bucket, prefix + '/validation')) // 2\n",
    "print ( ' Num validation Images = ' + str(num_validation_samples))\n",
    "\n",
    "num_validation_annotation_samples = len(sess.list_s3_files(bucket, prefix + '/validation_annotation'))\n",
    "print ( ' Num validation annotation Images = ' + str(num_validation_annotation_samples))\n",
    "\n",
    "assert num_validation_samples == num_validation_annotation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create channel names for the s3 bucket.\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'\n",
    "# label_map_channel = prefix + '/label_map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sagemaker estimator object.\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p3.2xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = 'pipeline-deeplab',\n",
    "                                         sagemaker_session = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters \n",
    "ss_model.set_hyperparameters(backbone='resnet-50', # This is the encoder. Options include resnet-50 and resnet-101\n",
    "                             algorithm='deeplab', # This is the decoder. Options include 'psp', 'fcn' and 'deeplab'                             \n",
    "                             use_pretrained_model='True', # Use the pre-trained model.\n",
    "                             crop_size=240, # Size of image random crop.                             \n",
    "                             num_classes=2, # This is a mandatory parameter.\n",
    "                             epochs=30, # Number of epochs to run.\n",
    "                             learning_rate=0.0001,                             \n",
    "                             optimizer='rmsprop', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler='poly', # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=16, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=8,\n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=5, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_min_epochs=10, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=num_training_samples) # This is a mandatory parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full bucket names\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)\n",
    "\n",
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data logs will also print out training loss on the training data, which is the pixel-wise cross-entropy loss as described in the algorithm papers. The data logs will also print out pixel-wise label accuracy and mean intersection-over-union (mIoU) on the validation data after a run of the dataset once or one epoch. These metrics measure the quality of the model under training.\n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as output_path in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hosting\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same instance (or type of instance) that we used to train. Training is a prolonged and compute heavy job that require a different of compute and memory requirements that hosting typically do not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_predictor = ss_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, we can use this endpoint for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test.jpg'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "\n",
    "# resize image size for inference\n",
    "im = PIL.Image.open(filename)\n",
    "im.thumbnail([800,600],PIL.Image.ANTIALIAS)\n",
    "im.save(filename, \"JPEG\")\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(im)\n",
    "plt.axis('off')\n",
    "with open(filename, 'rb') as image:\n",
    "    img = image.read()\n",
    "    img = bytearray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The endpoint accepts images in formats similar to the ones found images in the training dataset. It accepts the image/jpeg content_type. The accept parameter takes on two values: image/png and application/x-protobuf. For customers who want an indexed-PNG segmentation mask such as the ones that were used during training, can use the image/png accept type as shown in the example below. Using this endpoint will return a image bytearray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "ss_predictor.content_type = 'image/jpeg'\n",
    "ss_predictor.accept = 'image/png'\n",
    "return_img = ss_predictor.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "num_classes = 2\n",
    "mask = np.array(Image.open(io.BytesIO(return_img)))\n",
    "plt.imshow(mask, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second accept type allows us to request all the class probabilities for each pixels. Let us use our endpoint to try to predict the probabilites of segments within this image. Since the image is jpeg, we use the appropriate content_type to run the prediction job. The endpoint returns a file that we can simply load and peek into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# resize image size for inference\n",
    "im = PIL.Image.open(filename)\n",
    "im.thumbnail([800,600],PIL.Image.ANTIALIAS)\n",
    "im.save(filename, \"JPEG\")\n",
    "with open(filename, 'rb') as image:\n",
    "    img = image.read()\n",
    "    img = bytearray(img)\n",
    "    \n",
    "ss_predictor.content_type = 'image/jpeg'\n",
    "ss_predictor.accept = 'application/x-protobuf'\n",
    "results = ss_predictor.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we receive back is a recordio-protobuf of probablities sent as a binary. It takes a little bit of effort to convert into a readable array. Let us convert them to numpy format. We can make use of mxnet that has the capability to read recordio-protobuf formats. Using this, we can convert the outcoming bytearray into numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.record_pb2 import Record\n",
    "import mxnet as mx\n",
    "\n",
    "results_file = 'results.rec'\n",
    "with open(results_file, 'wb') as f:\n",
    "    f.write(results)\n",
    "\n",
    "rec = Record()\n",
    "recordio = mx.recordio.MXRecordIO(results_file, 'r')\n",
    "protobuf = rec.ParseFromString(recordio.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The protobuf array has two parts to it. The first part contains the shape of the output and the second contains the values of probabilites. Using the output shape, we can transform the probabilities into the shape of the image, so that we get a map of values. There typically is a singleton dimension since we are only inferring on one image. We can also remove that using the squeeze method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(rec.features[\"target\"].float32_tensor.values)\n",
    "shape = list(rec.features[\"shape\"].int32_tensor.values)\n",
    "shape = np.squeeze(shape)\n",
    "mask = np.reshape(np.array(values), shape)\n",
    "mask = np.squeeze(mask, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as to plot the segmentation mask from the list of probabilities, let us get the indices of the most probable class for each pixel. We can do this by measuring the argmax across the classes axis of the probability data. To plot the probabilites as image, we can use the numpy.argmax method to find out which probabilities are the largest and plot only those as a segmentaiton mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_map = np.argmax(mask, axis=0)\n",
    "num_classes = 2\n",
    "plt.imshow(pred_map, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ss_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jeremyjordan.me/evaluating-image-segmentation-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
