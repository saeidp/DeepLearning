{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "#import torchnet as tnt\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'saeid-cat-dog'\n",
    "\n",
    "outputpath='s3://saeid-cat-dog/output'\n",
    "training_path = 's3://saeid-cat-dog/data'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script\n",
    "\n",
    "Show the entry point script inline for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function, division\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.distributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn.functional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data.distributed\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m lr_scheduler\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms, models\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m OrderedDict\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcopy\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtnt\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "validation_loss=[]\r\n",
      "min_val_loss=\u001b[34m9999\u001b[39;49;00m\r\n",
      "train_accuracy=[]\r\n",
      "validation_accuracy=[]\r\n",
      "\u001b[37m#MEAN = [0.413, 0.413, 0.413]\u001b[39;49;00m\r\n",
      "\u001b[37m#STD = [0.159, 0.161, 0.158]\u001b[39;49;00m\r\n",
      "\r\n",
      "MEAN = [\u001b[34m0.485\u001b[39;49;00m, \u001b[34m0.456\u001b[39;49;00m, \u001b[34m0.406\u001b[39;49;00m]\r\n",
      "STD = [\u001b[34m0.229\u001b[39;49;00m, \u001b[34m0.224\u001b[39;49;00m, \u001b[34m0.225\u001b[39;49;00m]\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_average_gradients\u001b[39;49;00m(model):\r\n",
      "    \u001b[37m# Gradient averaging.\u001b[39;49;00m\r\n",
      "    size = \u001b[36mfloat\u001b[39;49;00m(dist.get_world_size())\r\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\r\n",
      "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        param.grad.data /= size\r\n",
      "        \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_init_weights\u001b[39;49;00m(model):\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(model, nn.Conv2d):\r\n",
      "        torch.nn.init.xavier_uniform_(model.weight)\r\n",
      "        \u001b[34mif\u001b[39;49;00m model.bias \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
      "            model.bias.data.fill_(\u001b[34m0.01\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(model, nn.BatchNorm2d):\r\n",
      "        model.weight.data.uniform_()\r\n",
      "        \u001b[34mif\u001b[39;49;00m model.bias \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
      "            model.bias.data.zero_()        \r\n",
      "        \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_model\u001b[39;49;00m(model_def,NUM_CLASSES,model_location):\r\n",
      "    \r\n",
      "    pretrained_resent=\u001b[36mFalse\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m model_location==\u001b[36mNone\u001b[39;49;00m:\r\n",
      "        pretrained_resent=\u001b[36mTrue\u001b[39;49;00m\r\n",
      "     \r\n",
      "    \u001b[34mif\u001b[39;49;00m model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mresnet18\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        model_ft = models.resnet18(pretrained=pretrained_resent)\r\n",
      "    \u001b[34melif\u001b[39;49;00m model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mresnet34\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        model_ft = models.resnet34(pretrained=pretrained_resent)\r\n",
      "    \u001b[34melif\u001b[39;49;00m model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mresnet50\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        model_ft = models.resnet50(pretrained=pretrained_resent)\r\n",
      "    \u001b[34melif\u001b[39;49;00m model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mresnet101\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        model_ft = models.resnet101(pretrained=pretrained_resent)\r\n",
      "    \u001b[34melif\u001b[39;49;00m model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mdensenet121\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        model_ft = models.densenet121(pretrained=pretrained_resent)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mChoose valid model def...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m#if pretrained_resent:\u001b[39;49;00m\r\n",
      "        \u001b[37m#for param in model_ft.parameters():\u001b[39;49;00m\r\n",
      "            \u001b[37m#param.requires_grad = False\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mresnet18\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mresnet34\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mresnet50\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mresnet101\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        model_ft.avgpool = nn.AdaptiveAvgPool2d(\u001b[34m1\u001b[39;49;00m)\r\n",
      "        num_ftrs = model_ft.fc.in_features\r\n",
      "        model_ft.fc = nn.Sequential(OrderedDict([\r\n",
      "                                            (\u001b[33m'\u001b[39;49;00m\u001b[33mfc1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nn.Linear(num_ftrs, \u001b[34m256\u001b[39;49;00m)),\r\n",
      "                                            (\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nn.ReLU()),\r\n",
      "                                            (\u001b[33m'\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nn.Dropout(\u001b[34m0.2\u001b[39;49;00m)),\r\n",
      "                                            (\u001b[33m'\u001b[39;49;00m\u001b[33mfc2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nn.Linear(\u001b[34m256\u001b[39;49;00m, NUM_CLASSES))\r\n",
      "                                            ]))\r\n",
      "    \u001b[37m# The classifier part is a single fully-connected layer (classifier): Linear(in_features=1024, out_features=1000).\u001b[39;49;00m\r\n",
      "    \u001b[37m# we substitute it by or nn.Sequential \u001b[39;49;00m\r\n",
      "                                    \r\n",
      "    \u001b[34mif\u001b[39;49;00m  model_def == \u001b[33m'\u001b[39;49;00m\u001b[33mdensenet121\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        model_ft.avgpool = nn.AdaptiveAvgPool2d(\u001b[34m1\u001b[39;49;00m)\r\n",
      "        num_ftrs = model_ft.classifier.in_features\r\n",
      "        model_ft.classifier = nn.Sequential(OrderedDict([\r\n",
      "                                            (\u001b[33m'\u001b[39;49;00m\u001b[33mfc1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nn.Linear(num_ftrs, \u001b[34m256\u001b[39;49;00m)),\r\n",
      "                                            (\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nn.ReLU()),\r\n",
      "                                            (\u001b[33m'\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nn.Dropout(\u001b[34m0.2\u001b[39;49;00m)),\r\n",
      "                                            (\u001b[33m'\u001b[39;49;00m\u001b[33mfc2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nn.Linear(\u001b[34m256\u001b[39;49;00m, NUM_CLASSES))\r\n",
      "                                            ]))\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m pretrained_resent:\r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded pretrained model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        model_ft =torch.nn.DataParallel(model_ft)\r\n",
      "        tar = tarfile.open(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mr:gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mfor\u001b[39;49;00m member \u001b[35min\u001b[39;49;00m tar.getmembers():\r\n",
      "            \u001b[34mif\u001b[39;49;00m member.name==\u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "                f=tar.extractfile(member)\r\n",
      "                \u001b[37m#original saved file with DataParallel\u001b[39;49;00m\r\n",
      "                model_ft.load_state_dict(torch.load(f))\r\n",
      "        \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model_ft\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msgdr_lr\u001b[39;49;00m(max_lr, min_lr, cycle_length, current_step):\r\n",
      "    current_step = current_step % cycle_length\r\n",
      "    max_min_dist = max_lr - min_lr\r\n",
      "    cos_arg = ((math.pi / \u001b[34m2\u001b[39;49;00m) / cycle_length) * (current_step - \u001b[34m1\u001b[39;49;00m)\r\n",
      "    dist_penalty = \u001b[34m1\u001b[39;49;00m - math.cos(cos_arg)\r\n",
      "    new_lr = max_lr - (max_min_dist * dist_penalty)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m new_lr\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransfer_learning\u001b[39;49;00m(args):\r\n",
      "    \u001b[33m'''Training function'''\u001b[39;49;00m\r\n",
      "    min_val_loss=\u001b[34m9999\u001b[39;49;00m\r\n",
      "    is_distributed = \u001b[36mlen\u001b[39;49;00m(args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDistributed training - {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_distributed))\r\n",
      "    \r\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.num_gpus))\r\n",
      "    kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\r\n",
      "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\r\n",
      "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\r\n",
      "        os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\r\n",
      "        host_rank = args.hosts.index(args.current_host)\r\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInitialized the distributed environment: \u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33m backend on {} nodes. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\r\n",
      "            args.backend, dist.get_world_size()) + \u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host rank is {}. Number of gpus: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\r\n",
      "            dist.get_rank(), args.num_gpus))\r\n",
      "\r\n",
      "    device = \u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mDevice Type: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\r\n",
      "    \r\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\r\n",
      "        torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "    \u001b[37m# Create data loader objects from our helper functions\u001b[39;49;00m\r\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining data set curation start:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    logger.info(datetime.datetime.now())\r\n",
      "    \r\n",
      "    train_loader = Get_train_data_loader(args.batch_size, args.data_dir+\u001b[33m'\u001b[39;49;00m\u001b[33m/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).get_data_loader()\r\n",
      "    valid_loader = Get_validation_data_loader(args.batch_size, args.data_dir+\u001b[33m'\u001b[39;49;00m\u001b[33m/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).get_data_loader()\r\n",
      "    test_loader = Get_test_data_loader(args.batch_size, args.data_dir+\u001b[33m'\u001b[39;49;00m\u001b[33m/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).get_data_loader()\r\n",
      "\r\n",
      "   \r\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining data set curation end:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    logger.info(datetime.datetime.now())\r\n",
      "    \r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses {}/{} ({:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler), \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\r\n",
      "        \u001b[34m100.\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\r\n",
      "    ))\r\n",
      "\r\n",
      "    \u001b[37m# Load model and send to device\u001b[39;49;00m\r\n",
      "    model = _get_model(args.model_def,args.no_of_classes,args.model_location)\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.model_location \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m use_cuda:\r\n",
      "            \u001b[37m# multi-machine multi-gpu case\u001b[39;49;00m\r\n",
      "            model = torch.nn.parallel.DistributedDataParallel(model)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\r\n",
      "            model = torch.nn.DataParallel(model)\r\n",
      "        \r\n",
      "    model = model.to(device)\r\n",
      "    \r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel on cuda: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[36mnext\u001b[39;49;00m(model.parameters()).is_cuda))    \r\n",
      "    \r\n",
      "    criterion = nn.CrossEntropyLoss().to(device)\r\n",
      "    \r\n",
      "    \u001b[37m# Optimize using SGD, notice every parameter is being trained!\u001b[39;49;00m\r\n",
      "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\r\n",
      "    \r\n",
      "    \u001b[37m# Learning rate scheduler, cutting lr by 10 every arg.lr_step\u001b[39;49;00m\r\n",
      "    \u001b[37m#scheduler = lr_scheduler.StepLR(optimizer, step_size=args.lr_step, gamma=0.1)\u001b[39;49;00m\r\n",
      "    scheduler = \u001b[36mNone\u001b[39;49;00m\r\n",
      "    \r\n",
      "    iters = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):     \r\n",
      "        \u001b[37m#Training\u001b[39;49;00m\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch {}/{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch, args.epochs))\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m * \u001b[34m10\u001b[39;49;00m)\r\n",
      "        model, epoch_loss, epoch_acc, iters = train(train_loader, model, device, scheduler, optimizer, iters, args.lr, epoch,\r\n",
      "                                             criterion, is_distributed, use_cuda)\r\n",
      "        \r\n",
      "        train_accuracy.append(epoch_acc)\r\n",
      "        \r\n",
      "         \u001b[37m# log the information of the epoch\u001b[39;49;00m\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain accuracy: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_accuracy))\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Loss: {:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch_loss))\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Acc: {:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch_acc))\r\n",
      "\r\n",
      "        \u001b[37m# Validation step\u001b[39;49;00m\r\n",
      "        epoch_loss, epoch_acc, conf_matrix, wrong_pred_paths = validate(model, valid_loader, args.no_of_classes, args.batch_size, device)\r\n",
      "        \r\n",
      "        validation_accuracy.append(epoch_acc)\r\n",
      "        validation_loss.append(epoch_loss)\r\n",
      "        \r\n",
      "        \u001b[37m# log validation information\u001b[39;49;00m\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mValidation accuracy: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_accuracy))\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mVal Loss: {:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch_loss))\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mVal Acc: {:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch_acc))\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[37m# Log learning rate\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m param_group \u001b[35min\u001b[39;49;00m optimizer.param_groups:\r\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mLearning rate: {:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(param_group[\u001b[33m'\u001b[39;49;00m\u001b[33mlr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "            \r\n",
      "        \u001b[37m# log Confusion matrix for epoch\u001b[39;49;00m\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mConfusion matrix: \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(conf_matrix.value()))\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "          \r\n",
      "        \u001b[37m# log wrong prediction paths\u001b[39;49;00m\r\n",
      "        \u001b[37m#logger.info('Incorrect prediction paths: {}'.format(wrong_pred_paths))\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m epoch_loss<min_val_loss:\r\n",
      "            min_val_loss=epoch_loss\r\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mstoring best model Loss: {:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch_loss))\r\n",
      "            best_model=copy.deepcopy(model)\r\n",
      "        \r\n",
      "        \u001b[37m# This at least needs 4 epochs\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(validation_loss)>\u001b[34m4\u001b[39;49;00m:\r\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal iterations: {:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(iters))\r\n",
      "            previous_loss=validation_loss[-\u001b[34m4\u001b[39;49;00m]\r\n",
      "            min_loss=\u001b[36mmin\u001b[39;49;00m(validation_loss[-\u001b[34m3\u001b[39;49;00m:])\r\n",
      "            max_loss=\u001b[36mmax\u001b[39;49;00m(validation_loss[-\u001b[34m3\u001b[39;49;00m:])\r\n",
      "            \r\n",
      "            \u001b[34mif\u001b[39;49;00m previous_loss<min_loss \u001b[35mor\u001b[39;49;00m \u001b[36mabs\u001b[39;49;00m(max_loss-min_loss)<\u001b[34m0.01\u001b[39;49;00m:\r\n",
      "                logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mValidation loss is increasing, training more doesnt increase accuracy and causes overfitting\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "                \u001b[34mbreak\u001b[39;49;00m\r\n",
      "           \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFinished Training\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStart Testing\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    test(best_model, test_loader, args.no_of_classes, args.batch_size, device, criterion, use_cuda)\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFinished Testing\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Save model\u001b[39;49;00m\r\n",
      "    save_model(best_model, args.model_dir)\r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(train_loader, model, device, scheduler, optimizer, iters, lr, epoch, criterion, is_distributed, use_cuda):\r\n",
      "    \r\n",
      "        \u001b[37m# make sure model in training mode (batch norm only in training!)\u001b[39;49;00m\r\n",
      "        model.train()\r\n",
      "        \r\n",
      "        \u001b[37m# step scheduler\u001b[39;49;00m\r\n",
      "        \u001b[37m#scheduler.step()\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[37m# init statistics we care about tracking\u001b[39;49;00m\r\n",
      "        running_loss = \u001b[34m0.0\u001b[39;49;00m\r\n",
      "        running_corrects = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[37m# batch loop, we loop over train_loader object\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target, paths) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\r\n",
      "            iters += \u001b[34m1\u001b[39;49;00m\r\n",
      "            \r\n",
      "            update_lr = sgdr_lr(lr / (\u001b[34m2\u001b[39;49;00m ** (epoch // \u001b[34m10\u001b[39;49;00m)), .\u001b[34m001\u001b[39;49;00m, \u001b[34m100\u001b[39;49;00m, iters)\r\n",
      "            \u001b[34mfor\u001b[39;49;00m param_group \u001b[35min\u001b[39;49;00m optimizer.param_groups:\r\n",
      "                param_group[\u001b[33m'\u001b[39;49;00m\u001b[33mlr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = update_lr     \r\n",
      "            \r\n",
      "            data, target = data.to(device), target.to(device)\r\n",
      "           \r\n",
      "            \u001b[37m# zero out gradients\u001b[39;49;00m\r\n",
      "            optimizer.zero_grad()\r\n",
      "                 \r\n",
      "            \u001b[37m# get outputs of model on current data and calculate prediction\u001b[39;49;00m\r\n",
      "            output = model(data)\r\n",
      "            \r\n",
      "            _, preds = torch.max(output, \u001b[34m1\u001b[39;49;00m)\r\n",
      "            \r\n",
      "            \u001b[37m# calculate the CrossEntropyLoss for output and backpropagate error\u001b[39;49;00m\r\n",
      "            loss = criterion(output, target)\r\n",
      "            \r\n",
      "            loss.backward()\r\n",
      "            \r\n",
      "            \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m use_cuda:\r\n",
      "                \u001b[37m# average gradients manually for multi-machine cpu case only\u001b[39;49;00m\r\n",
      "                _average_gradients(model)\r\n",
      "                \r\n",
      "            optimizer.step()\r\n",
      "            \r\n",
      "            \u001b[37m# statistics\u001b[39;49;00m\r\n",
      "            running_loss += loss.item() * data.size(\u001b[34m0\u001b[39;49;00m)\r\n",
      "            running_corrects += torch.sum(preds == target.data)\r\n",
      "        \r\n",
      "        \u001b[37m# find the epoch's loss and acc using running numbers divided by data set size\u001b[39;49;00m\r\n",
      "        epoch_loss = running_loss / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\r\n",
      "        epoch_acc = running_corrects.double() / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\r\n",
      "        \r\n",
      "        \u001b[34mreturn\u001b[39;49;00m model, epoch_loss, epoch_acc, iters\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mvalidate\u001b[39;49;00m(model, valid_loader, NUM_CLASSES,batch_size, device):\r\n",
      "    \u001b[33m'''Vaidation function'''\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[37m# set model to eval mode\u001b[39;49;00m\r\n",
      "    model.eval()\r\n",
      "    \r\n",
      "    \u001b[37m# init statistics\u001b[39;49;00m\r\n",
      "    conf_matrix = tnt.meter.ConfusionMeter(NUM_CLASSES)\r\n",
      "    \r\n",
      "    running_loss = \u001b[34m0.0\u001b[39;49;00m\r\n",
      "    running_corrects = \u001b[34m0\u001b[39;49;00m\r\n",
      "    wrong_pred_paths = []\r\n",
      "   \r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target, paths \u001b[35min\u001b[39;49;00m valid_loader:\r\n",
      "            \r\n",
      "            data, target = data.to(device), target.to(device)\r\n",
      "                    \r\n",
      "            output = model(data)\r\n",
      "            \r\n",
      "            _, preds = torch.max(output, \u001b[34m1\u001b[39;49;00m)\r\n",
      "            loss = nn.CrossEntropyLoss()(output, target)\r\n",
      "                        \r\n",
      "            conf_matrix.add(output.data, target.data)\r\n",
      "            running_loss += loss.item() * data.size(\u001b[34m0\u001b[39;49;00m)\r\n",
      "            running_corrects += torch.sum(preds == target.data)\r\n",
      "            \r\n",
      "            \u001b[37m# logging incorrect prediction file names\u001b[39;49;00m\r\n",
      "            \u001b[34mfor\u001b[39;49;00m ind, num \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m((preds == target.data)):\r\n",
      "                \u001b[34mif\u001b[39;49;00m num == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                    filepath = paths[ind]\r\n",
      "                    wrong_pred_paths.append((filepath, output[ind].cpu().numpy()))\r\n",
      "    \r\n",
      "    \u001b[37m# get epoch stats by dividing running stats by data size\u001b[39;49;00m\r\n",
      "    epoch_loss = running_loss / \u001b[36mlen\u001b[39;49;00m(valid_loader.dataset)\r\n",
      "    epoch_acc = running_corrects.double() / \u001b[36mlen\u001b[39;49;00m(valid_loader.dataset)\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m epoch_loss,epoch_acc,conf_matrix,wrong_pred_paths\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, NUM_CLASSES, batch_size, device, criterion, use_cuda):\r\n",
      "    \u001b[37m# track test loss\u001b[39;49;00m\r\n",
      "    test_loss = \u001b[34m0.0\u001b[39;49;00m\r\n",
      "    test_correct = \u001b[34m0.0\u001b[39;49;00m\r\n",
      "    wrong_pred_paths = []\r\n",
      "    class_correct = \u001b[36mlist\u001b[39;49;00m(\u001b[34m0.\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(NUM_CLASSES))\r\n",
      "    class_total = \u001b[36mlist\u001b[39;49;00m(\u001b[34m0.\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(NUM_CLASSES))\r\n",
      "    model.eval()\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target, paths \u001b[35min\u001b[39;49;00m test_loader:            \r\n",
      "            data, target = data.to(device), target.to(device)                    \r\n",
      "            output = model(data)\r\n",
      "            \u001b[37m# calculate the batch loss\u001b[39;49;00m\r\n",
      "            loss = criterion(output, target)\r\n",
      "            \u001b[37m# update test loss\u001b[39;49;00m\r\n",
      "            test_loss += loss.item()*data.size(\u001b[34m0\u001b[39;49;00m)\r\n",
      "            \u001b[37m# convert output probabilities to predicted class\u001b[39;49;00m\r\n",
      "            _, preds = torch.max(output, \u001b[34m1\u001b[39;49;00m)\r\n",
      "            \u001b[37m# compare predictions to true label\u001b[39;49;00m\r\n",
      "            correct_tensor = preds.eq(target.data.view_as(preds))\r\n",
      "            test_correct = np.squeeze(correct_tensor.numpy()) \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m np.squeeze(correct_tensor.cpu().numpy())\r\n",
      "            \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(data)):\r\n",
      "                label = target.data[i]\r\n",
      "                class_correct[label] += test_correct[i].item()\r\n",
      "                class_total[label] += \u001b[34m1\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[37m# average test loss\u001b[39;49;00m\r\n",
      "        test_loss = test_loss/\u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTest Loss: {:.6f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_loss))\r\n",
      "        \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(NUM_CLASSES):\r\n",
      "            \u001b[34mif\u001b[39;49;00m class_total[i] > \u001b[34m0\u001b[39;49;00m:\r\n",
      "                logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTest Accuracy of \u001b[39;49;00m\u001b[33m%5s\u001b[39;49;00m\u001b[33m: \u001b[39;49;00m\u001b[33m%2d\u001b[39;49;00m\u001b[33m%%\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m%2d\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%2d\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (\r\n",
      "                    classes[i], \u001b[34m100\u001b[39;49;00m * class_correct[i] / class_total[i],\r\n",
      "                    np.sum(class_correct[i]), np.sum(class_total[i])))\r\n",
      "            \u001b[34melse\u001b[39;49;00m:\r\n",
      "                logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTest Accuracy of \u001b[39;49;00m\u001b[33m%5s\u001b[39;49;00m\u001b[33m: N/A (no training examples)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (classes[i]))\r\n",
      "\r\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTest Accuracy (Overall): \u001b[39;49;00m\u001b[33m%2d\u001b[39;49;00m\u001b[33m%%\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m%2d\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%2d\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (\r\n",
      "            \u001b[34m100.\u001b[39;49;00m * np.sum(class_correct) / np.sum(class_total),\r\n",
      "            np.sum(class_correct), np.sum(class_total)))   \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model to {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_dir))\r\n",
      "    path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\r\n",
      "    \u001b[37m#torch.save(model.cpu().state_dict(), path)\u001b[39;49;00m\r\n",
      "    \u001b[37m#torch.save(model.cpu().module.state_dict(), path)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# save full model - model.module will be the method to predict image\u001b[39;49;00m\r\n",
      "    \u001b[37m#torch.save(model, path)\u001b[39;49;00m\r\n",
      "    \u001b[37m# save the part inside parallel data \u001b[39;49;00m\r\n",
      "    torch.save(model.cpu().module, path)\r\n",
      "\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#------------------------------------ Data Loader ---------------------------------------------\u001b[39;49;00m\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "MEAN = [\u001b[34m0.413\u001b[39;49;00m, \u001b[34m0.413\u001b[39;49;00m, \u001b[34m0.413\u001b[39;49;00m]\r\n",
      "STD = [\u001b[34m0.159\u001b[39;49;00m, \u001b[34m0.161\u001b[39;49;00m, \u001b[34m0.158\u001b[39;49;00m]\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMake_weights_for_balanced_classes\u001b[39;49;00m():\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,images,nclasses):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.images = images\r\n",
      "        \u001b[36mself\u001b[39;49;00m.nclasses = nclasses\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_weights\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        count = [\u001b[34m0\u001b[39;49;00m] * \u001b[36mself\u001b[39;49;00m.nclasses                                                      \r\n",
      "        \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.images:                                                         \r\n",
      "            count[item[\u001b[34m1\u001b[39;49;00m]] += \u001b[34m1\u001b[39;49;00m  \u001b[37m# item is (img-data, label-id)\u001b[39;49;00m\r\n",
      "        weight_per_class = [\u001b[34m0.\u001b[39;49;00m] * \u001b[36mself\u001b[39;49;00m.nclasses                                      \r\n",
      "        N = \u001b[36mfloat\u001b[39;49;00m(\u001b[36msum\u001b[39;49;00m(count))  \u001b[37m# total number of images                  \u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.nclasses):                                                   \r\n",
      "            weight_per_class[i] = N/\u001b[36mfloat\u001b[39;49;00m(count[i])                                 \r\n",
      "        weight = [\u001b[34m0\u001b[39;49;00m] * \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.images)                                              \r\n",
      "        \u001b[34mfor\u001b[39;49;00m idx, val \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.images):                                          \r\n",
      "            weight[idx] = weight_per_class[val[\u001b[34m1\u001b[39;49;00m]]     \r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m weight\r\n",
      "   \r\n",
      " \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mImageFolderWithPaths\u001b[39;49;00m(datasets.ImageFolder):\r\n",
      "    \u001b[33m\"\"\"Custom dataset that includes image file paths. Extends\u001b[39;49;00m\r\n",
      "\u001b[33m    torchvision.datasets.ImageFolder\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# override the __getitem__ method. this is the method dataloader calls\u001b[39;49;00m\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, index):\r\n",
      "        \u001b[37m# this is what ImageFolder normally returns \u001b[39;49;00m\r\n",
      "        original_tuple = \u001b[36msuper\u001b[39;49;00m(ImageFolderWithPaths, \u001b[36mself\u001b[39;49;00m).\u001b[32m__getitem__\u001b[39;49;00m(index)\r\n",
      "        \u001b[37m# the image file path\u001b[39;49;00m\r\n",
      "        path = \u001b[36mself\u001b[39;49;00m.imgs[index][\u001b[34m0\u001b[39;49;00m]\r\n",
      "        \u001b[37m# make a new tuple that includes original and the path\u001b[39;49;00m\r\n",
      "        tuple_with_path = (original_tuple + (path,))\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tuple_with_path\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mGet_train_data_loader\u001b[39;49;00m():\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, batch_size,training_dir):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = batch_size\r\n",
      "        \u001b[36mself\u001b[39;49;00m.training_dir = training_dir\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_data_loader\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "        data_transform = transforms.Compose([\r\n",
      "            \u001b[37m# randomly change the brightness, contrast and saturation\u001b[39;49;00m\r\n",
      "            \u001b[37m# factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] and the same for others\u001b[39;49;00m\r\n",
      "            \u001b[37m# but hue (float or tuple of float (min, max)): How much to jitter hue.\u001b[39;49;00m\r\n",
      "            \u001b[37m# hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].\u001b[39;49;00m\r\n",
      "            \u001b[37m# Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.\u001b[39;49;00m\r\n",
      "            transforms.ColorJitter(brightness=\u001b[34m0.3\u001b[39;49;00m, contrast=\u001b[34m0.3\u001b[39;49;00m, saturation=\u001b[34m0.25\u001b[39;49;00m, hue=\u001b[34m0.1\u001b[39;49;00m),\r\n",
      "            \r\n",
      "            \u001b[37m# The transform RandomResizedCrop crops the input image by a random size(within a scale range of 0.8 to 1.0\u001b[39;49;00m\r\n",
      "            \u001b[37m# of the original size and a random aspect ratio in the default range of 0.75 to 1.33 ).\u001b[39;49;00m\r\n",
      "            \u001b[37m#The crop is then resized to 256×256            \u001b[39;49;00m\r\n",
      "            transforms.RandomResizedCrop(size=\u001b[34m256\u001b[39;49;00m, scale=(\u001b[34m0.8\u001b[39;49;00m, \u001b[34m1.0\u001b[39;49;00m)),\r\n",
      "            transforms.RandomRotation(degrees=\u001b[34m15\u001b[39;49;00m),\r\n",
      "            transforms.RandomHorizontalFlip(),\r\n",
      "            transforms.RandomVerticalFlip(),\r\n",
      "            transforms.CenterCrop(size=\u001b[34m224\u001b[39;49;00m),\r\n",
      "            transforms.ToTensor(),\r\n",
      "            transforms.Normalize(mean=MEAN, std=STD)\r\n",
      "        ])\r\n",
      "\r\n",
      "        dataset = ImageFolderWithPaths(\u001b[36mself\u001b[39;49;00m.training_dir, data_transform)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(dataset)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(dataset.classes)\r\n",
      "\r\n",
      "        weights = Make_weights_for_balanced_classes(\r\n",
      "                    dataset.imgs, \u001b[36mlen\u001b[39;49;00m(dataset.classes)).get_weights()\r\n",
      "\r\n",
      "        sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, \u001b[36mlen\u001b[39;49;00m(weights))\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(dataset, batch_size=\u001b[36mself\u001b[39;49;00m.batch_size, sampler = sampler)\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mGet_test_data_loader\u001b[39;49;00m():\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, test_batch_size,testing_dir):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = test_batch_size\r\n",
      "        \u001b[36mself\u001b[39;49;00m.testing_dir = testing_dir\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_data_loader\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet test data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        data_transform = transforms.Compose([\r\n",
      "            \r\n",
      "            transforms.Resize(size=\u001b[34m256\u001b[39;49;00m),\r\n",
      "            transforms.CenterCrop(size=\u001b[34m224\u001b[39;49;00m),\r\n",
      "            transforms.ToTensor(),\r\n",
      "            transforms.Normalize(mean=MEAN, std=STD)\r\n",
      "        ])\r\n",
      "\r\n",
      "        dataset = ImageFolderWithPaths(\u001b[36mself\u001b[39;49;00m.testing_dir, data_transform)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(dataset, batch_size=\u001b[36mself\u001b[39;49;00m.batch_size, shuffle=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mGet_validation_data_loader\u001b[39;49;00m():\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, validation_batch_size,validation_dir):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = validation_batch_size\r\n",
      "        \u001b[36mself\u001b[39;49;00m.validation_dir = validation_dir\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_data_loader\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet validation data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        data_transform = transforms.Compose([\r\n",
      "            transforms.Resize(size=\u001b[34m256\u001b[39;49;00m),\r\n",
      "            transforms.CenterCrop(size=\u001b[34m224\u001b[39;49;00m),\r\n",
      "            transforms.ToTensor(),\r\n",
      "            transforms.Normalize(mean=MEAN, std=STD)\r\n",
      "        ])\r\n",
      "\r\n",
      "        dataset = ImageFolderWithPaths(\u001b[36mself\u001b[39;49;00m.validation_dir, data_transform)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(dataset, batch_size=\u001b[36mself\u001b[39;49;00m.batch_size, shuffle=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    \r\n",
      "    \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m#------------------------------ Run --------------------------------------------------\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_def\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mresnet18\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mthe base resnet model to use resnet(18, 34, 50)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 16)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 100)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.001)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr_step\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate scheduler step size (default: 50)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.9)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data_location\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[36mNone\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe location where the data is located\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_location\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[36mNone\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mThe location where the model is located\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--mean\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=[\u001b[34m0.325\u001b[39;49;00m,\u001b[34m0.315\u001b[39;49;00m,\u001b[34m0.130\u001b[39;49;00m], metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mME\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mAvg image mean (default: [0.325,0.315,0.130])\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--std\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=[\u001b[34m0.104\u001b[39;49;00m, \u001b[34m0.134\u001b[39;49;00m, \u001b[34m0.153\u001b[39;49;00m], metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mStd of images (default: [0.104, 0.134, 0.153])\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--no_of_classes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNo of images events (default: 2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mSE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[36mNone\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    env = sagemaker_containers.training_env()\r\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining started:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    logger.info(datetime.datetime.now())\r\n",
      "    logger.info(env.channel_input_dirs.get(\u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    \r\n",
      "    inbuilt=env.channel_input_dirs.get(\u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    args=parser.parse_args()\r\n",
      "    model_data = {}  \r\n",
      "    \r\n",
      "    model_data[\u001b[33m'\u001b[39;49;00m\u001b[33mBaseModel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]=args.model_def\r\n",
      "    \r\n",
      "    classes=[]\r\n",
      "    folders=[]\r\n",
      "    \r\n",
      "    model_meta_data=os.path.join(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mmodelMetaData.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "   \r\n",
      "    \u001b[37m#Creating a list of images used for training,validation and testing\u001b[39;49;00m\r\n",
      "    train_path=os.path.join(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mTrainImages.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the training images to {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_path))\r\n",
      "    train_text_file = \u001b[36mopen\u001b[39;49;00m(train_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    train_text_file.write(\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Images:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    train_text_file.write(\u001b[33m'\u001b[39;49;00m\u001b[33m\\r\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mfor\u001b[39;49;00m dirpath, dirnames, filenames \u001b[35min\u001b[39;49;00m os.walk(env.channel_input_dirs.get(\u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)+\u001b[33m'\u001b[39;49;00m\u001b[33m/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "        train_images_list=[f \u001b[34mfor\u001b[39;49;00m f \u001b[35min\u001b[39;49;00m filenames \u001b[34mif\u001b[39;49;00m f.endswith((\u001b[33m\"\u001b[39;49;00m\u001b[33m.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33m.jpg\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))]\r\n",
      "        \u001b[37m#logger.info(train_images_list)\u001b[39;49;00m\r\n",
      "        \r\n",
      "        folders+=[dirpath]\r\n",
      "        dirct=dirpath.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        classes=classes+dirnames\r\n",
      "        \u001b[34mfor\u001b[39;49;00m filename \u001b[35min\u001b[39;49;00m train_images_list:\r\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(dirct)>\u001b[34m6\u001b[39;49;00m:\r\n",
      "                prefix=(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).join(dirct[-\u001b[34m2\u001b[39;49;00m:])\r\n",
      "                train_text_file.write(os.path.join(args.data_location,prefix, filename))\r\n",
      "                \r\n",
      "            \u001b[37m#classes.add(filename.split('/')[0])\u001b[39;49;00m\r\n",
      "            train_text_file.write(\u001b[33m'\u001b[39;49;00m\u001b[33m\\r\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    train_text_file.close()\r\n",
      "                            \r\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mEvent name:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    logger.info(classes)  \r\n",
      "    \r\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mFolders name:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    logger.info(folders)\r\n",
      "    classes.sort()\r\n",
      "                            \r\n",
      "    events={v: k \u001b[34mfor\u001b[39;49;00m v, k \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(classes)}\r\n",
      "                            \r\n",
      "    model_data[\u001b[33m'\u001b[39;49;00m\u001b[33mClasses\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]=\u001b[36mlen\u001b[39;49;00m(classes)\r\n",
      "    model_data[\u001b[33m'\u001b[39;49;00m\u001b[33mLabels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]=events\r\n",
      "    \r\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mEvents mapping: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    logger.info(events)\r\n",
      "    \r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_meta_data, \u001b[33m'\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m outfile:  \r\n",
      "        json.dump(model_data, outfile)\r\n",
      "                            \r\n",
      "    test_path=os.path.join(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mTestImages.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the Test images to {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_path))\r\n",
      "    test_text_file = \u001b[36mopen\u001b[39;49;00m(test_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    test_text_file.write(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest Images:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    test_text_file.write(\u001b[33m'\u001b[39;49;00m\u001b[33m\\r\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m dirpath, dirnames, filenames \u001b[35min\u001b[39;49;00m os.walk(env.channel_input_dirs.get(\u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)+\u001b[33m'\u001b[39;49;00m\u001b[33m/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "            dirct=dirpath.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[34mfor\u001b[39;49;00m filename \u001b[35min\u001b[39;49;00m [f \u001b[34mfor\u001b[39;49;00m f \u001b[35min\u001b[39;49;00m filenames \u001b[34mif\u001b[39;49;00m f.endswith((\u001b[33m\"\u001b[39;49;00m\u001b[33m.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33m.jpg\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))]:\r\n",
      "                \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(dirct)>\u001b[34m6\u001b[39;49;00m:\r\n",
      "                    prefix=(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).join(dirct[-\u001b[34m2\u001b[39;49;00m:])\r\n",
      "                    test_text_file.write(os.path.join(args.data_location,prefix, filename))\r\n",
      "                test_text_file.write(\u001b[33m'\u001b[39;49;00m\u001b[33m\\r\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    test_text_file.close()\r\n",
      "    \r\n",
      "    val_path=os.path.join(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mValidationImages.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the validation images to {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(val_path))\r\n",
      "    val_text_file = \u001b[36mopen\u001b[39;49;00m(val_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    val_text_file.write(\u001b[33m\"\u001b[39;49;00m\u001b[33mValidation Images:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    val_text_file.write(\u001b[33m'\u001b[39;49;00m\u001b[33m\\r\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m dirpath, dirnames, filenames \u001b[35min\u001b[39;49;00m os.walk(env.channel_input_dirs.get(\u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)+\u001b[33m'\u001b[39;49;00m\u001b[33m/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "            dirct=dirpath.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)                    \r\n",
      "            \u001b[34mfor\u001b[39;49;00m filename \u001b[35min\u001b[39;49;00m [f \u001b[34mfor\u001b[39;49;00m f \u001b[35min\u001b[39;49;00m filenames \u001b[34mif\u001b[39;49;00m f.endswith((\u001b[33m\"\u001b[39;49;00m\u001b[33m.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33m.jpg\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))]:\r\n",
      "                \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(dirct)>\u001b[34m6\u001b[39;49;00m:\r\n",
      "                    prefix=(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).join(dirct[-\u001b[34m2\u001b[39;49;00m:])                \r\n",
      "                    val_text_file.write(os.path.join(args.data_location,\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, filename))\r\n",
      "                val_text_file.write(\u001b[33m'\u001b[39;49;00m\u001b[33m\\r\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    val_text_file.close()\r\n",
      "    \r\n",
      "    model_dir_location=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODULE_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    model_artifacts=model_dir_location.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m#logger.info(('/').join(model_artifacts))\u001b[39;49;00m\r\n",
      "    \r\n",
      "    resource = boto3.resource(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    my_bucket = resource.Bucket(model_artifacts[\u001b[34m2\u001b[39;49;00m]) \r\n",
      "    \r\n",
      "    my_bucket.download_file((\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).join(model_artifacts[\u001b[34m3\u001b[39;49;00m:]),\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/model/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m+model_artifacts[-\u001b[34m1\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    \u001b[37m#logger.info(model_dir_location)\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m args.model_location!=\u001b[36mNone\u001b[39;49;00m:\r\n",
      "        model_artifacts=args.model_location.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        my_bucket = resource.Bucket(model_artifacts[\u001b[34m2\u001b[39;49;00m]) \r\n",
      "        model_path=(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).join(model_artifacts[\u001b[34m3\u001b[39;49;00m:])\r\n",
      "        output_model_path=model_artifacts[-\u001b[34m1\u001b[39;49;00m]\r\n",
      "        my_bucket.download_file(model_path,output_model_path)\r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel is Downloaded\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    transfer_learning(args)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Create PyTorch estimator\n",
    "\n",
    "Instantiate a sagemaker estimator object and pass it the entry point script and path to save the model\n",
    "\n",
    "This is also where you may define non-default hyperparameters found in the entry point script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    output_path=outputpath,\n",
    "                    base_job_name='resnet50-cat-dog',\n",
    "                    train_volume_size=20,\n",
    "                    role=role,\n",
    "                    framework_version='1.0', \n",
    "                    train_instance_count=1,\n",
    "                    train_max_run=(5*24*60*60),\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    #train_instance_type='ml.p2.8xlarge',\n",
    "                    hyperparameters={\n",
    "                        'batch_size': 64,\n",
    "                        'lr': 0.01,\n",
    "                        'lr_step': 10,\n",
    "                        'data_location':training_path,\n",
    "                        'epochs':5,\n",
    "                        'mean': [0.485, 0.456, 0.406],\n",
    "                        'std':[0.229, 0.224, 0.225],\n",
    "                        'no_of_classes':2,             \n",
    "                        'model_def':'resnet50'\n",
    "                    }) # hyperparameters gets replaced with a config file using SGD with warm start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-16 08:39:15 Starting - Starting the training job...\n",
      "2020-04-16 08:39:17 Starting - Launching requested ML instances......\n",
      "2020-04-16 08:40:16 Starting - Preparing the instances for training......\n",
      "2020-04-16 08:41:18 Downloading - Downloading input data......\n",
      "2020-04-16 08:42:34 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-04-16 08:42:56,145 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-04-16 08:42:56,172 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-04-16 08:42:57,586 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-04-16 08:42:57,797 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-04-16 08:42:57,797 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-04-16 08:42:57,797 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-04-16 08:42:57,798 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-anqc_2fp/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-04-16 08:42:59,678 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"no_of_classes\": 2,\n",
      "        \"std\": [\n",
      "            0.229,\n",
      "            0.224,\n",
      "            0.225\n",
      "        ],\n",
      "        \"batch_size\": 64,\n",
      "        \"model_def\": \"resnet50\",\n",
      "        \"lr\": 0.01,\n",
      "        \"data_location\": \"s3://saeid-cat-dog/data\",\n",
      "        \"mean\": [\n",
      "            0.485,\n",
      "            0.456,\n",
      "            0.406\n",
      "        ],\n",
      "        \"epochs\": 5,\n",
      "        \"lr_step\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"resnet50-cat-dog-2020-04-16-08-39-14-910\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://saeid-cat-dog/resnet50-cat-dog-2020-04-16-08-39-14-910/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":64,\"data_location\":\"s3://saeid-cat-dog/data\",\"epochs\":5,\"lr\":0.01,\"lr_step\":10,\"mean\":[0.485,0.456,0.406],\"model_def\":\"resnet50\",\"no_of_classes\":2,\"std\":[0.229,0.224,0.225]}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://saeid-cat-dog/resnet50-cat-dog-2020-04-16-08-39-14-910/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":64,\"data_location\":\"s3://saeid-cat-dog/data\",\"epochs\":5,\"lr\":0.01,\"lr_step\":10,\"mean\":[0.485,0.456,0.406],\"model_def\":\"resnet50\",\"no_of_classes\":2,\"std\":[0.229,0.224,0.225]},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"resnet50-cat-dog-2020-04-16-08-39-14-910\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://saeid-cat-dog/resnet50-cat-dog-2020-04-16-08-39-14-910/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"64\",\"--data_location\",\"s3://saeid-cat-dog/data\",\"--epochs\",\"5\",\"--lr\",\"0.01\",\"--lr_step\",\"10\",\"--mean\",\"[0.485, 0.456, 0.406]\",\"--model_def\",\"resnet50\",\"--no_of_classes\",\"2\",\"--std\",\"[0.229, 0.224, 0.225]\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_NO_OF_CLASSES=2\u001b[0m\n",
      "\u001b[34mSM_HP_STD=[0.229,0.224,0.225]\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DEF=resnet50\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_LOCATION=s3://saeid-cat-dog/data\u001b[0m\n",
      "\u001b[34mSM_HP_MEAN=[0.485,0.456,0.406]\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_LR_STEP=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --batch_size 64 --data_location s3://saeid-cat-dog/data --epochs 5 --lr 0.01 --lr_step 10 --mean [0.485, 0.456, 0.406] --model_def resnet50 --no_of_classes 2 --std [0.229, 0.224, 0.225]\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mTraining started:\u001b[0m\n",
      "\u001b[34mTraining started:\u001b[0m\n",
      "\u001b[34m2020-04-16 08:43:01.131431\u001b[0m\n",
      "\u001b[34m2020-04-16 08:43:01.131431\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34mDevice Type: cuda\u001b[0m\n",
      "\u001b[34mDevice Type: cuda\u001b[0m\n",
      "\u001b[34mTraining data set curation start:\u001b[0m\n",
      "\u001b[34mTraining data set curation start:\u001b[0m\n",
      "\u001b[34m2020-04-16 08:43:01.324961\u001b[0m\n",
      "\u001b[34m2020-04-16 08:43:01.324961\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mDataset ImageFolderWithPaths\n",
      "    Number of datapoints: 5915\n",
      "    Root Location: /opt/ml/input/data/training/train\n",
      "    Transforms (if any): Compose(\n",
      "                             ColorJitter(brightness=0.3, contrast=0.3, saturation=0.25, hue=0.1)\n",
      "                             RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
      "                             RandomRotation(degrees=(-15, 15), resample=False, expand=False)\n",
      "                             RandomHorizontalFlip(p=0.5)\n",
      "                             RandomVerticalFlip(p=0.5)\n",
      "                             CenterCrop(size=(224, 224))\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=[0.413, 0.413, 0.413], std=[0.159, 0.161, 0.158])\n",
      "                         )\n",
      "    Target Transforms (if any): None\u001b[0m\n",
      "\u001b[34m['cat', 'dog']\u001b[0m\n",
      "\u001b[34mGet validation data loader\u001b[0m\n",
      "\u001b[34mGet validation data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mTraining data set curation end:\u001b[0m\n",
      "\u001b[34mTraining data set curation end:\u001b[0m\n",
      "\u001b[34m2020-04-16 08:43:01.363169\u001b[0m\n",
      "\u001b[34m2020-04-16 08:43:01.363169\u001b[0m\n",
      "\u001b[34mProcesses 5915/5915 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 5915/5915 (100%) of train data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-04-16 08:42:54 Training - Training image download completed. Training in progress.\u001b[34mModel on cuda: True\u001b[0m\n",
      "\u001b[34mModel on cuda: True\u001b[0m\n",
      "\u001b[34mEpoch 1/5\u001b[0m\n",
      "\u001b[34mEpoch 1/5\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.1606\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.1606\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9352\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9352\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0260\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0260\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9930\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9930\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0021\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0021\u001b[0m\n",
      "\u001b[34mstoring best model Loss: 0.0260\u001b[0m\n",
      "\u001b[34mstoring best model Loss: 0.0260\u001b[0m\n",
      "\u001b[34mEpoch 2/5\u001b[0m\n",
      "\u001b[34mEpoch 2/5\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64), tensor(0.9828, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64), tensor(0.9828, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.0499\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.0499\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9828\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9828\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64), tensor(0.9902, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64), tensor(0.9902, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0307\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0307\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9902\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9902\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0031\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0031\u001b[0m\n",
      "\u001b[34mEpoch 3/5\u001b[0m\n",
      "\u001b[34mEpoch 3/5\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64), tensor(0.9828, device='cuda:0', dtype=torch.float64), tensor(0.9887, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64), tensor(0.9828, device='cuda:0', dtype=torch.float64), tensor(0.9887, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.0274\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.0274\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9887\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9887\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64), tensor(0.9902, device='cuda:0', dtype=torch.float64), tensor(0.9937, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64), tensor(0.9902, device='cuda:0', dtype=torch.float64), tensor(0.9937, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0160\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0160\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9937\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9937\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0040\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0040\u001b[0m\n",
      "\u001b[34mstoring best model Loss: 0.0160\u001b[0m\n",
      "\u001b[34mstoring best model Loss: 0.0160\u001b[0m\n",
      "\u001b[34mEpoch 4/5\u001b[0m\n",
      "\u001b[34mEpoch 4/5\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64), tensor(0.9828, device='cuda:0', dtype=torch.float64), tensor(0.9887, device='cuda:0', dtype=torch.float64), tensor(0.9866, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64), tensor(0.9828, device='cuda:0', dtype=torch.float64), tensor(0.9887, device='cuda:0', dtype=torch.float64), tensor(0.9866, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.0349\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.0349\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9866\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9866\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64), tensor(0.9902, device='cuda:0', dtype=torch.float64), tensor(0.9937, device='cuda:0', dtype=torch.float64), tensor(0.9958, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64), tensor(0.9902, device='cuda:0', dtype=torch.float64), tensor(0.9937, device='cuda:0', dtype=torch.float64), tensor(0.9958, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0154\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0154\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9958\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9958\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0050\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0050\u001b[0m\n",
      "\u001b[34mstoring best model Loss: 0.0154\u001b[0m\n",
      "\u001b[34mstoring best model Loss: 0.0154\u001b[0m\n",
      "\u001b[34mEpoch 5/5\u001b[0m\n",
      "\u001b[34mEpoch 5/5\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34m----------\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64), tensor(0.9828, device='cuda:0', dtype=torch.float64), tensor(0.9887, device='cuda:0', dtype=torch.float64), tensor(0.9866, device='cuda:0', dtype=torch.float64), tensor(0.9919, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain accuracy: [tensor(0.9352, device='cuda:0', dtype=torch.float64), tensor(0.9828, device='cuda:0', dtype=torch.float64), tensor(0.9887, device='cuda:0', dtype=torch.float64), tensor(0.9866, device='cuda:0', dtype=torch.float64), tensor(0.9919, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.0239\u001b[0m\n",
      "\u001b[34mTrain Loss: 0.0239\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9919\u001b[0m\n",
      "\u001b[34mTrain Acc: 0.9919\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64), tensor(0.9902, device='cuda:0', dtype=torch.float64), tensor(0.9937, device='cuda:0', dtype=torch.float64), tensor(0.9958, device='cuda:0', dtype=torch.float64), tensor(0.9958, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mValidation accuracy: [tensor(0.9930, device='cuda:0', dtype=torch.float64), tensor(0.9902, device='cuda:0', dtype=torch.float64), tensor(0.9937, device='cuda:0', dtype=torch.float64), tensor(0.9958, device='cuda:0', dtype=torch.float64), tensor(0.9958, device='cuda:0', dtype=torch.float64)]\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0123\u001b[0m\n",
      "\u001b[34mVal Loss: 0.0123\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9958\u001b[0m\n",
      "\u001b[34mVal Acc: 0.9958\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0058\u001b[0m\n",
      "\u001b[34mLearning rate: 0.0058\u001b[0m\n",
      "\u001b[34mstoring best model Loss: 0.0123\u001b[0m\n",
      "\u001b[34mstoring best model Loss: 0.0123\u001b[0m\n",
      "\u001b[34mtotal iterations: 465.0000\u001b[0m\n",
      "\u001b[34mtotal iterations: 465.0000\u001b[0m\n",
      "\u001b[34mValidation loss is increasing, training more doesnt increase accuracy and causes overfitting\u001b[0m\n",
      "\u001b[34mValidation loss is increasing, training more doesnt increase accuracy and causes overfitting\u001b[0m\n",
      "\u001b[34mFinished Training\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mStart Testing\u001b[0m\n",
      "\u001b[34mTest Loss: 0.000305\u001b[0m\n",
      "\u001b[34mTest Loss: 0.000305\u001b[0m\n",
      "\u001b[34mTest Accuracy of   dog: 100% ( 5/ 5)\u001b[0m\n",
      "\u001b[34mTest Accuracy of   dog: 100% ( 5/ 5)\u001b[0m\n",
      "\u001b[34mTest Accuracy of   cat: 100% ( 5/ 5)\u001b[0m\n",
      "\u001b[34mTest Accuracy of   cat: 100% ( 5/ 5)\u001b[0m\n",
      "\u001b[34mTest Accuracy (Overall): 100% (10/10)\u001b[0m\n",
      "\u001b[34mTest Accuracy (Overall): 100% (10/10)\u001b[0m\n",
      "\u001b[34mFinished Testing\n",
      "\u001b[0m\n",
      "\u001b[34mSaving the model to /opt/ml/model.\u001b[0m\n",
      "\u001b[34mSaving the model to /opt/ml/model.\u001b[0m\n",
      "\u001b[34m2020-04-16 09:02:49,342 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-16 09:03:03 Uploading - Uploading generated training model\n",
      "2020-04-16 09:03:20 Completed - Training job completed\n",
      "Training seconds: 1322\n",
      "Billable seconds: 1322\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': training_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
